// ğŸš€ ã‚¦ãƒ«ãƒˆãƒ©ã‚·ãƒ³ã‚¯ ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
const { Worker, isMainThread, parentPort, workerData } = require('worker_threads');
const { dbPool, executeQuery } = require('../config/database');
const ultraCache = require('./ultra-cache');

class UltraBatchProcessor {
  constructor() {
    this.maxWorkers = Math.min(require('os').cpus().length, 8);
    this.activeWorkers = new Set();
    this.taskQueue = [];
    this.processingStats = {
      totalProcessed: 0,
      averageTime: 0,
      lastBatchTime: null
    };
  }

  // ğŸ”¥ å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å‡¦ç†
  async processBulkOrders(orders, options = {}) {
    const {
      batchSize = 1000,
      maxRetries = 3,
      enableCache = true,
      parallel = true
    } = options;

    console.log(`ğŸš€ Bulk Order Processing Started: ${orders.length} orders`);
    const startTime = Date.now();

    try {
      if (parallel && orders.length > batchSize * 2) {
        return await this.processParallel(orders, batchSize, 'orders');
      } else {
        return await this.processSequential(orders, batchSize, 'orders');
      }
    } catch (error) {
      console.error('Bulk processing failed:', error);
      throw error;
    } finally {
      const duration = Date.now() - startTime;
      this.updateStats(orders.length, duration);
      console.log(`ğŸ”¥ Bulk Processing Completed in ${duration}ms`);
    }
  }

  // ğŸš€ ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
  async processParallel(data, batchSize, type) {
    const chunks = this.chunkArray(data, batchSize);
    const numWorkers = Math.min(this.maxWorkers, chunks.length);
    
    console.log(`ğŸ”¥ Parallel Processing: ${chunks.length} batches across ${numWorkers} workers`);

    const workerPromises = [];
    const workersPerBatch = Math.ceil(chunks.length / numWorkers);

    for (let i = 0; i < numWorkers; i++) {
      const workerChunks = chunks.slice(i * workersPerBatch, (i + 1) * workersPerBatch);
      if (workerChunks.length > 0) {
        workerPromises.push(this.createWorker({
          chunks: workerChunks,
          type,
          workerId: i
        }));
      }
    }

    const results = await Promise.all(workerPromises);
    return results.flat();
  }

  // ğŸ”¥ ãƒ¯ãƒ¼ã‚«ãƒ¼ä½œæˆ
  createWorker(data) {
    return new Promise((resolve, reject) => {
      const worker = new Worker(__filename, {
        workerData: data
      });

      this.activeWorkers.add(worker);

      worker.on('message', (result) => {
        this.activeWorkers.delete(worker);
        resolve(result);
      });

      worker.on('error', (error) => {
        this.activeWorkers.delete(worker);
        reject(error);
      });

      worker.on('exit', (code) => {
        this.activeWorkers.delete(worker);
        if (code !== 0) {
          reject(new Error(`Worker stopped with exit code ${code}`));
        }
      });
    });
  }

  // ğŸš€ é †æ¬¡å‡¦ç†å®Ÿè¡Œ
  async processSequential(data, batchSize, type) {
    const batches = this.chunkArray(data, batchSize);
    const results = [];

    for (let i = 0; i < batches.length; i++) {
      console.log(`ğŸ”¥ Processing batch ${i + 1}/${batches.length}`);
      const batchResult = await this.processBatch(batches[i], type);
      results.push(...batchResult);

      // ãƒ¡ãƒ¢ãƒªç®¡ç†ã®ãŸã‚ã®å°ä¼‘æ­¢
      if (i % 10 === 0 && i > 0) {
        await this.sleep(100);
      }
    }

    return results;
  }

  // ğŸ”¥ å˜ä¸€ãƒãƒƒãƒå‡¦ç†
  async processBatch(batch, type) {
    switch (type) {
      case 'orders':
        return await this.processOrderBatch(batch);
      case 'products':
        return await this.processProductBatch(batch);
      case 'inventory':
        return await this.processInventoryBatch(batch);
      default:
        throw new Error(`Unknown batch type: ${type}`);
    }
  }

  // ğŸš€ æ³¨æ–‡ãƒãƒƒãƒå‡¦ç†
  async processOrderBatch(orders) {
    const sql = `
      INSERT INTO orders (user_id, product_id, quantity, unit_price, total_amount, status, created_at)
      VALUES ($1, $2, $3, $4, $5, $6, $7)
      ON CONFLICT (id) DO UPDATE SET
        status = EXCLUDED.status,
        updated_at = NOW()
      RETURNING id, status
    `;

    const results = [];
    const client = await dbPool.connect();

    try {
      await client.query('BEGIN');

      for (const order of orders) {
        const values = [
          order.user_id,
          order.product_id,
          order.quantity,
          order.unit_price,
          order.total_amount,
          order.status || 'pending',
          order.created_at || new Date()
        ];

        const result = await client.query(sql, values);
        results.push(result.rows[0]);

        // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–
        await ultraCache.invalidate(`user_orders_${order.user_id}`);
      }

      await client.query('COMMIT');
      console.log(`ğŸ”¥ Order batch processed: ${results.length} orders`);
      return results;

    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  }

  // ğŸš€ å•†å“ãƒãƒƒãƒå‡¦ç†
  async processProductBatch(products) {
    const sql = `
      INSERT INTO products (name, description, price, category_id, stock_quantity, status)
      VALUES ($1, $2, $3, $4, $5, $6)
      ON CONFLICT (sku) DO UPDATE SET
        price = EXCLUDED.price,
        stock_quantity = EXCLUDED.stock_quantity,
        updated_at = NOW()
      RETURNING id, name, price
    `;

    const results = [];
    for (const product of products) {
      const values = [
        product.name,
        product.description,
        product.price,
        product.category_id,
        product.stock_quantity,
        product.status || 'active'
      ];

      const result = await executeQuery(sql, values);
      results.push(result.rows[0]);

      // å•†å“ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
      await ultraCache.set(`product_${product.id}`, result.rows[0], 'product');
    }

    console.log(`ğŸ”¥ Product batch processed: ${results.length} products`);
    return results;
  }

  // ğŸš€ åœ¨åº«ãƒãƒƒãƒå‡¦ç†
  async processInventoryBatch(inventoryItems) {
    const sql = `
      UPDATE inventory 
      SET quantity = $2, updated_at = NOW()
      WHERE product_id = $1
      RETURNING product_id, quantity
    `;

    const results = [];
    for (const item of inventoryItems) {
      const result = await executeQuery(sql, [item.product_id, item.quantity]);
      results.push(result.rows[0]);

      // åœ¨åº«ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
      await ultraCache.invalidate(`inventory_${item.product_id}`);
    }

    console.log(`ğŸ”¥ Inventory batch processed: ${results.length} items`);
    return results;
  }

  // ğŸ”¥ é…åˆ—ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
  chunkArray(array, chunkSize) {
    const chunks = [];
    for (let i = 0; i < array.length; i += chunkSize) {
      chunks.push(array.slice(i, i + chunkSize));
    }
    return chunks;
  }

  // ğŸš€ çµ±è¨ˆæ›´æ–°
  updateStats(processed, duration) {
    this.processingStats.totalProcessed += processed;
    this.processingStats.averageTime = 
      (this.processingStats.averageTime + duration) / 2;
    this.processingStats.lastBatchTime = new Date();
  }

  // ğŸ”¥ çµ±è¨ˆå–å¾—
  getStats() {
    return {
      ...this.processingStats,
      activeWorkers: this.activeWorkers.size,
      maxWorkers: this.maxWorkers,
      queueSize: this.taskQueue.length
    };
  }

  // ğŸš€ ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
  sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// ğŸ”¥ ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†
if (!isMainThread) {
  const { chunks, type, workerId } = workerData;
  
  (async () => {
    try {
      console.log(`ğŸš€ Worker ${workerId} started: ${chunks.length} chunks`);
      const processor = new UltraBatchProcessor();
      const results = [];

      for (const chunk of chunks) {
        const result = await processor.processBatch(chunk, type);
        results.push(...result);
      }

      console.log(`ğŸ”¥ Worker ${workerId} completed: ${results.length} processed`);
      parentPort.postMessage(results);
    } catch (error) {
      console.error(`Worker ${workerId} error:`, error);
      parentPort.postMessage({ error: error.message });
    }
  })();
}

module.exports = UltraBatchProcessor;